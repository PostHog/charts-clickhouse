# Default values for posthog.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  # -- Posthog image repository
  repository: posthog/posthog
  # -- Posthog image sha, e.g. sha256:20af35fca6756d689d6705911a49dd6f2f6631e001ad43377b605cfc7c133eb4
  sha:
  # -- Posthog image tag, e.g. release-1.25.0
  tag:
  # -- Default image or tag
  default: "@sha256:20af35fca6756d689d6705911a49dd6f2f6631e001ad43377b605cfc7c133eb4"
  # -- Image pull policy
  pullPolicy: IfNotPresent

# -- Required: Cloud service being deployed on. Either `gcp` or `aws` or `do` for DigitalOcean
cloud:
# -- Sentry endpoint to send errors to
sentryDSN:

clickhouseOperator:
  # -- Whether to install clickhouse. If false, `clickhouse.host` must be set
  enabled: true
  # -- Which namespace to install clickhouse operator to (defaults to namespace chart is installed to)
  namespace:
  # -- How much storage space to preallocate for clickhouse
  storage: 20Gi
  # -- If enabled, operator will prefer k8s nodes with tag `clickhouse:true`
  useNodeSelector: false
  # -- Service Type: LoadBalancer (allows external access) or NodePort (more secure, no extra cost)
  serviceType: NodePort


# -- Env vars to throw into every deployment (web, beat, worker, and plugin server)
env:
  - name: ASYNC_EVENT_PROPERTY_USAGE
    value: "true"
  - name: EVENT_PROPERTY_USAGE_INTERVAL_SECONDS
    value: "86400"

# -- PgBouncer setup
pgbouncer:
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for pgbouncer
    # -- This experimental and set up based on cpu utilization
    # -- Adding pgbouncers can cause running out of connections for Postgres
    enabled: false
    # -- CPU threshold percent for pgbouncer
    cputhreshold: 60
    # -- Min pods for pgbouncer
    minpods: 1
    # -- Max pods for pgbouncer
    maxpods: 10
  # -- How many replicas of pgbouncer to run. Ignored if hpa is used
  replicacount: 1
  # -- Additional env vars to be added to the pgbouncer deployment
  env: []

web:
  # -- Web horizontal pod autoscaler settings
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for web
    # -- This experimental
    enabled: false
    # -- CPU threshold percent for the web
    cputhreshold: 60
    # -- Min pods for the web
    minpods: 1
    # -- Max pods for the web
    maxpods: 10
  # -- Amount of web pods to run. Ignored if hpa is used
  replicacount: 1
  # -- Resource limits for web service. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container for more
  resources:
    {}
    # limits:
    #   cpu: 500m
    #   memory: 500Mi
    # requests:
    #   cpu: 300m
    #   memory: 300Mi
  # -- Env variables for web container
  env:
    # -- Set google oauth 2 key. Requires posthog ee license.
    - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
      value:
    # -- Set google oauth 2 secret. Requires posthog ee license.
    - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
      value:
    # -- Set google oauth 2 whitelisted domains users can log in from
    - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
      value: "posthog.com"

  internalMetrics:
    # -- Whether to capture information on operation of posthog into posthog, exposed in /instance/status page
    capture: true

  # -- Node labels for web pod assignment
  nodeSelector: {}
  # -- Toleration labels for web pod assignment
  tolerations: []
  # -- Affinity settings for web pod assignment
  affinity: {}
  # :TODO:
  secureCookies: true
  livenessProbe:
    # -- The liveness probe failure threshold
    failureThreshold: 5
    # -- The liveness probe initial delay seconds
    initialDelaySeconds: 50
    # -- The liveness probe period seconds
    periodSeconds: 10
    # -- The liveness probe success threshold
    successThreshold: 1
    # -- The liveness probe timeout seconds
    timeoutSeconds: 2
  readinessProbe:
    # -- The readiness probe failure threshold
    failureThreshold: 10
    # -- The readiness probe initial delay seconds
    initialDelaySeconds: 50
    # -- The readiness probe period seconds
    periodSeconds: 10
    # -- The readiness probe success threshold
    successThreshold: 1
    # -- The readiness probe timeout seconds
    timeoutSeconds: 2
  # priorityClassName: ""
  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []

beat:
  # -- How many posthog 'beat' instances to run
  replicacount: 1
  # -- Resource limits for 'beat' instances
  resources:
    {}
    # limits:
    #   cpu: 200m
    #   memory: 200Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # priorityClassName: ""
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []

worker:
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for worker
    # -- This experimental
    enabled: false
    cputhreshold: 60
    minpods: 1
    maxpods: 20
  env: []
  # -- How many replicas of workers to run. Ignored if hpa is used
  replicacount: 1
  # -- Resource limits for workers
  resources:
    {}
    # limits:
    #   cpu: 300m
    #   memory: 500Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # priorityClassName: ""
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  # concurrency:

# How many plugin server instances to run
plugins:
  ingestion:
    # -- Whether to enable plugin-server based ingestion
    enabled: true
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for plugin server
    # -- This experimental, based on cpu util which is not necessarilly the bottleneck
    enabled: false
    cputhreshold: 60
    minpods: 1
    maxpods: 10
  env: []
  # -- How many replicas of plugin-server to run. Ignored if hpa is used
  replicacount: 1
  resources:
    {}
    # limits:
    #   cpu: 300m
    #   memory: 500Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # priorityClassName: ""
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  # concurrency:

# Outbound E-mails
email:
  # -- Outbound email sender
  from_email:
  # -- STMP host
  host:
  # -- STMP port
  port:
  # -- STMP login user
  user:
  # -- STMP password
  password:
  # -- SMTP TLS for security
  use_tls: true
  # -- SMTP SSL for security
  use_ssl:
  # -- SMTP password from an existing secret. When defined the `password` field is ignored
  existingSecret:
  # -- Key to get from the `email.existingSecret` secret
  existingSecretKey:
  # When defined the `password` field is ignored
  # existingSecret: secret-name
  # existingSecretKey: smtp-password

# -- Name of the service and what port to expose on the pod. Don't change these unless you know what you're doing
service:
  name: posthog
  type: NodePort
  externalPort: 8000
  internalPort: 8000

  ## Service annotations
  ##
  annotations: {}

  ## External IP addresses of service
  ## Default: nil
  ##
  # externalIPs:
  # - 192.168.0.1
  ## Load Balancer allow-list
  # loadBalancerSourceRanges: []

certManager:
  # -- Whether to install cert-manager. Validates certs for nginx ingress
  enabled: false

ingress:
  # -- Enable ingress controller resource
  enabled: true
  # -- Ingress handler type. Defaults to `nginx` if nginx is enabled and to `clb` on gcp.
  type:
  # -- URL to address your PostHog installation. You will need to set up DNS after installation
  hostname:
  gcp:
    # -- Specifies the name of the global IP address resource to be associated with the google clb
    ip_name: "posthog"
    # -- If true, will force a https redirect when accessed over http
    forceHttps: true
    # -- Specifies the name of the tls secret to be used by the ingress. If not specified a managed certificate will be generated.
    secretName: ""
  # -- Whether to enable letsencrypt. Defaults to true if hostname is defined and nginx and certManager are enabled otherwise false.
  letsencrypt:
  nginx:
    # -- Whether nginx is enabled
    enabled: false
    # -- Whether to redirect to TLS with nginx ingress.
    redirectToTLS: true

postgresql:
  # -- Install postgres server on kubernetes (see below)
  enabled: true
  # -- Name override for postgresql app
  nameOverride: posthog-postgresql
  # -- Postgresql database name
  postgresqlDatabase: posthog
  # -- Postgresql database username
  postgresqlUsername: postgres
  # -- Postgresql database password
  postgresqlPassword: postgres
  persistence:
    # -- Enable persistence using PVC
    enabled: true
    # -- PVC Storage Request for PostgreSQL volume
    size: 10Gi
  # -- Host postgres is accessible from. Only set when internal PG is disabled
  postgresqlHost:
  # -- Host postgres is accessible from. Only set when internal PG is disabled
  postgresqlPort:
  # postgresqlHost: postgres
  # postgresqlPort: 5432
  # When defined the `postgresqlPassword` field is ignored
  # existingSecret: secret-name
  # existingSecretKey: postgresql-password

redis:
  # -- Install redis server on kubernetes (see below)
  enabled: true
  # -- Name override for redis app
  nameOverride: posthog-redis

  # -- Either standalone or cluster.
  architecture: standalone

  auth:
    # -- Don't require password by default
    enabled: false
    # Just omit the password field if your redis cluster doesn't use password
    # password: redis
    # When defined the `password` field is ignored
    # existingSecret: secret-name
    # existingSecretKey: redis-password

  # -- Host redis is accessible from. Only set when internal redis is disabled
  host:
  # -- Password for redis. Only set when internal redis is disabled
  password:
  # -- Port redis is accessible from. Only set when internal redis is disabled
  port:

  master:
    persistence:
      # -- Enable persistence using PVC
      enabled: true
      # -- PVC Storage Request for Redis volume
      size: 5Gi

kafka:
  # -- Install kafka on kubernetes
  enabled: true
  # -- Name override for kafka app
  nameOverride: posthog-kafka
  # -- URL for kafka. Only set when internal kafka is disabled
  url:
  # -- Host for kafka. Only set when internal kafka is disabled
  host:
  # -- Port for kafka. Only set when internal kafka is disabled
  port:
  service:
    enabled: false
    type: NodePort
  persistence:
    # -- Enable persistence using PVC
    enabled: true
    # -- PVC Storage Request for kafka volume
    size: 5Gi
  # -- A size-based retention policy for logs
  # -- Should be less than kafka.persistence.size, ideally 70-80%
  logRetentionBytes: _4_000_000_000
  # -- The minimum age of a log file to be eligible for deletion due to age
  logRetentionHours: 24
  zookeeper:
    # -- Install zookeeper on kubernetes
    enabled: false
    # -- Name override for zookeeper app
  externalZookeeper:
    # -- URL for zookeeper. Only set when internal zookeeper is disabled
    # -- IF using default clickhouse zookeeper use <deployment-name>-posthog-zookeeper
    servers:
    - posthog-posthog-zookeeper:2181

zookeeper:
  # -- Install zookeeper on kubernetes
  enabled: true
  # -- Name override for zookeeper app
  nameOverride: posthog-zookeeper
  # -- replica count for zookeeper
  replicaCount: 3

clickhouse:
  # -- Use clickhouse as primary database
  enabled: true
  # -- Clickhouse database
  database: posthog
  # -- Clickhouse user
  user: admin
  # -- Clickhouse password
  password: a1f31e03-c88e-4ca6-a2df-ad49183d15d9
  # -- Set if not installing clickhouse operator
  host:
  # :TODO:
  replication: false
  secure: false
  verify: false
  async: false
  # externalZookeeper:
    # -- URL for zookeeper.
    # servers:
      # - host: posthog-posthog-zookeeper
        # port: 2181


## Prometheus Exporter / Metrics
##
metrics:
  # -- Start an exporter for posthog metrics
  enabled: false

  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)

  # -- Metrics pods livenessProbe settings
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

  # -- Metrics pods readinessProbe settings
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

  # -- Metrics resource requests/limits. See more at http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 100m
  #     memory: 100Mi

  # -- Node labels for metrics pod
  nodeSelector: {}
  # -- Toleration labels for metrics pod assignment
  tolerations: []
  # -- Affinity settings for metrics pod
  affinity: {}
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  service:
    # --  Kubernetes service type for metrics service
    type: ClusterIP
    # -- Additional labels for metrics service
    labels: {}

  image:
    # -- Metrics exporter image repository
    repository: prom/statsd-exporter
    # -- Metrics exporter image tag
    tag: v0.10.5
    # -- Metrics exporter image pull policy
    pullPolicy: IfNotPresent

  # Enable this if you're using https://github.com/coreos/prometheus-operator
  serviceMonitor:
    # -- if `true`, creates a Prometheus Operator ServiceMonitor (also requires `metrics.enabled` to be `true`)
    enabled: false
    # -- Optional namespace which Prometheus is running in
    namespace:
    # -- How frequently to scrape metrics (use by default, falling back to Prometheus' default)
    interval:
    ## Defaults to what's used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)
    ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)
    ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)
    # -- Default to kube-prometheus install (CoreOS recommended), but should be set according to Prometheus install
    selector:
      prometheus: kube-prometheus

cloudwatch:
  # -- Enable cloudwatch container insights to get logs and metrics on AWS
  enabled: false
  # -- AWS region
  region:
  # -- AWS EKS cluster name
  clusterName:
  # -- fluentBit configuration
  fluentBit:
    server: "On"
    port: 2020
    readHead: "On"
    readTail: "Off"

# Provide affinity for hooks if needed
hooks:
  # -- Affinity settings for hooks
  affinity: {}
  migrate:
    # -- Env variables for migate hooks
    env: []
    # -- Hook job resource limits/requests
    resources:
      {}

serviceAccount:
  # -- Configures if a ServiceAccount with this name should be created
  create: true
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  # -- name of the ServiceAccount to be used by access-controlled resources.
  # @default autogenerated
  name:
  # -- Configures annotation for the ServiceAccount
  annotations: {}

prometheus:
  # -- Whether to enable a minimal prometheus installation for getting alerts/monitoring the stack
  enabled: false

  alertmanager:
    # -- If false, alertmanager will not be installed
    enabled: true

    # -- alertmanager resource requests and limits
    resources:
      limits:
        cpu: 100m
      requests:
        cpu: 50m

  kubeStateMetrics:
    # -- If false, kube-state-metrics sub-chart will not be installed
    enabled: true

  nodeExporter:
    # -- If false, node-exporter will not be installed
    enabled: true

    # -- node-exporter resource limits & requests
    resources:
      limits:
        cpu: 100m
        memory: 50Mi
      requests:
        cpu: 50m
        memory: 30Mi

  pushgateway:
    # -- If false, pushgateway will not be installed
    enabled: false

  alertmanagerFiles:
    # -- alertmanager configuration rules. See https://prometheus.io/docs/alerting/latest/configuration/
    alertmanager.yml:
      global: {}
      receivers:
        - name: default-receiver
      route:
        group_by: [alertname]
        receiver: default-receiver

  serverFiles:
    # -- Alerts configuration, see https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
    alerting_rules.yml:
      groups:
        - name: Posthog alerts
          rules:
            # Alert for any pod that is down for >5 minutes.
            - alert: PodDown
              expr: up{job="kubernetes-pods"} == 0
              for: 1m
              labels:
                severity: alert
              annotations:
                summary: "Pod {{ $labels.kubernetes_pod_name }} down."
                description: "Pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} down for more than 5 minutes."
            - alert: PodFrequentlyRestarting
              expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
              for: 10m
              labels:
                severity: warning
              annotations:
                description:
                  Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}}
                  times within the last hour
                summary: Pod is restarting frequently
            # Requires nodeExporter.enabled
            - alert: VolumeRemainingCapacityLowTest
              expr: kubelet_volume_stats_used_bytes/kubelet_volume_stats_capacity_bytes >= 0.85
              for: 5m
              labels:
                severity: page
              annotations:
                description: "Persistent volume claim {{ $labels.persistentvolumeclaim }} disk usage is above 85% for past 5 minutes"
                summary: "Kubernetes {{ $labels.persistentvolumeclaim }} is full (host {{ $labels.kubernetes_io_hostname }})"

# -- Prometheus StatsD configuration, see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-statsd-exporter
statsd:
  enabled: false

  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /metrics
    prometheus.io/port: "9102"
