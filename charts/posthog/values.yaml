# Default values for posthog.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  # -- Posthog image repository
  repository: posthog/posthog
  # -- Posthog image sha, e.g. sha256:20af35fca6756d689d6705911a49dd6f2f6631e001ad43377b605cfc7c133eb4
  sha:
  # -- Posthog image tag, e.g. release-1.31.1
  tag:
  # -- Default image or tag, e.g. :release-1.31.1
  default: ":release-1.31.1"
  # -- Image pull policy
  pullPolicy: IfNotPresent

# -- Required: Cloud service being deployed on. Either `gcp` or `aws` or `do` for DigitalOcean
cloud:
# -- Sentry endpoint to send errors to
sentryDSN:

clickhouseOperator:
  # -- Whether to install clickhouse. If false, `clickhouse.host` must be set
  enabled: true
  # -- Which namespace to install clickhouse operator to (defaults to namespace chart is installed to)
  namespace:
  # -- How much storage space to preallocate for clickhouse
  storage: 20Gi
  # -- If enabled, operator will prefer k8s nodes with tag `clickhouse:true`
  useNodeSelector: false
  # -- Service Type: LoadBalancer (allows external access) or NodePort (more secure, no extra cost)
  serviceType: NodePort

# -- Env vars to throw into every deployment (web, worker, and plugin server)
env:
  - name: ASYNC_EVENT_PROPERTY_USAGE
    value: "true"
  - name: EVENT_PROPERTY_USAGE_INTERVAL_SECONDS
    value: "86400"

# -- PgBouncer setup
pgbouncer:
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for pgbouncer
    # -- This experimental and set up based on cpu utilization
    # -- Adding pgbouncers can cause running out of connections for Postgres
    enabled: false
    # -- CPU threshold percent for pgbouncer
    cputhreshold: 60
    # -- Min pods for pgbouncer
    minpods: 1
    # -- Max pods for pgbouncer
    maxpods: 10
  # -- How many replicas of pgbouncer to run. Ignored if hpa is used
  replicacount: 1
  # -- Additional env vars to be added to the pgbouncer deployment
  env: []
  # -- Additional volumeMounts to be added to the pgbouncer deployment
  extraVolumeMounts: []
  # -- Additional volumes to be added to the pgbouncer deployment
  extraVolumes: []

web:
  # -- Web horizontal pod autoscaler settings
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for web
    # -- This experimental
    enabled: false
    # -- CPU threshold percent for the web
    cputhreshold: 60
    # -- Min pods for the web
    minpods: 1
    # -- Max pods for the web
    maxpods: 10
  # -- Amount of web pods to run. Ignored if hpa is used
  replicacount: 1
  # -- Resource limits for web service. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container for more
  resources:
    {}
    # limits:
    #   cpu: 500m
    #   memory: 500Mi
    # requests:
    #   cpu: 300m
    #   memory: 300Mi
  # -- Env variables for web container
  env:
    # -- Set google oauth 2 key. Requires posthog ee license.
    - name: SOCIAL_AUTH_GOOGLE_OAUTH2_KEY
      value:
    # -- Set google oauth 2 secret. Requires posthog ee license.
    - name: SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET
      value:
    # -- Set google oauth 2 whitelisted domains users can log in from
    - name: SOCIAL_AUTH_GOOGLE_OAUTH2_WHITELISTED_DOMAINS
      value: "posthog.com"

  internalMetrics:
    # -- Whether to capture information on operation of posthog into posthog, exposed in /instance/status page
    capture: true

  # -- Node labels for web pod assignment
  nodeSelector: {}
  # -- Toleration labels for web pod assignment
  tolerations: []
  # -- Affinity settings for web pod assignment
  affinity: {}
  # :TODO:
  secureCookies: true
  livenessProbe:
    # -- The liveness probe failure threshold
    failureThreshold: 5
    # -- The liveness probe initial delay seconds
    initialDelaySeconds: 50
    # -- The liveness probe period seconds
    periodSeconds: 10
    # -- The liveness probe success threshold
    successThreshold: 1
    # -- The liveness probe timeout seconds
    timeoutSeconds: 2
  readinessProbe:
    # -- The readiness probe failure threshold
    failureThreshold: 10
    # -- The readiness probe initial delay seconds
    initialDelaySeconds: 50
    # -- The readiness probe period seconds
    periodSeconds: 10
    # -- The readiness probe success threshold
    successThreshold: 1
    # -- The readiness probe timeout seconds
    timeoutSeconds: 2
  # priorityClassName: ""
  ## Use an alternate scheduler, e.g. "stork".
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []

worker:
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for worker
    # -- This experimental
    enabled: false
    cputhreshold: 60
    minpods: 1
    maxpods: 20
  env: []
  # -- How many replicas of workers to run. Ignored if hpa is used
  replicacount: 1
  # -- Resource limits for workers
  resources:
    {}
    # limits:
    #   cpu: 300m
    #   memory: 500Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # priorityClassName: ""
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  # concurrency:

# How many plugin server instances to run
plugins:
  ingestion:
    # -- Whether to enable plugin-server based ingestion
    enabled: true
  hpa:
    # -- Boolean to create a HorizontalPodAutoscaler for plugin server
    # -- This experimental, based on cpu util which is not necessarily the bottleneck
    enabled: false
    cputhreshold: 60
    minpods: 1
    maxpods: 10
  env: []
  # -- How many replicas of plugin-server to run. Ignored if hpa is used
  replicacount: 1
  resources:
    {}
    # limits:
    #   cpu: 300m
    #   memory: 500Mi
    # requests:
    #   cpu: 100m
    #   memory: 100Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  # priorityClassName: ""
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  # concurrency:

# Outbound E-mails
email:
  # -- Outbound email sender
  from_email:
  # -- STMP host
  host:
  # -- STMP port
  port:
  # -- STMP login user
  user:
  # -- STMP password
  password:
  # -- SMTP TLS for security
  use_tls: true
  # -- SMTP SSL for security
  use_ssl:
  # -- SMTP password from an existing secret. When defined the `password` field is ignored
  existingSecret:
  # -- Key to get from the `email.existingSecret` secret
  existingSecretKey:
  # When defined the `password` field is ignored
  # existingSecret: secret-name
  # existingSecretKey: smtp-password

# SAML
saml:
  # -- Whether password-based login is disabled and users automatically redirected to SAML login. Requires SAML to be properly configured.
  enforced: false
  # -- Whether SAML should be completely disabled. If set at build time, this will also prevent SAML dependencies from being installed.
  disabled: false
  # -- Entity ID from your SAML IdP.
  # entity_id: "id-from-idp-5f9d4e-47ca-5080"
  entity_id:
  # -- Assertion Consumer Service URL from your SAML IdP.
  # acs_url: "https://mysamlidp.com/saml2"
  acs_url:
  # -- Public X509 certificate from your SAML IdP to validate SAML assertions
  # x509_cert: |
  # MIID3DCCAsSgAwIBAgIUdriHo8qmAU1I0gxsI7cFZHmna38wDQYJKoZIhvcNAQEF
  # BQAwRTEQMA4GA1UECgwHUG9zdEhvZzEVMBMGA1UECwwMT25lTG9naW4gSWRQMRow
  # GAYDVQQDDBFPbmVMb2dpbiBBY2NvdW50IDAeFw0yMTA4MTYyMTUyMzNaFw0yNjA4
  # MTYyMTUyMzNaMEUxEDAOBgNVBAoMB1Bvc3RIb2cxFTATBgNVBAsMDE9uZUxvZ2lu
  # IElkUDEaMBgGA1UEAwwRT25lTG9naW4gQWNjb3VudCAwggEiMA0GCSqGSIb3DQEB
  # AQUAA4IBDwAwggEKAoIBAQDEfUWFIU38ztF2EgijVsIbnlB8OIwkjZU8c34B9VwZ
  # BQQUSxbrkuT9AX/5O27G04TBCHFZsXRId+ABSjVo8daCPu0d38Quo9KS3V3627Nw
  # YcTYsje95lB02E/PgfiEQ6ZGCOV0P4xY9C99d26PoYTcoMT1S73jDDMOFtoD5WXG
  # ZsKqwBks1jbLkv6RYoFBlZX00aGzOXDzUXI59/0c15KR4EzgTad0t6CU7X0HZ2Qf
  # xGUiRb7hDLvgSby0SzpQpYUyYDnN9aSNYzpu1hiyIqrhQ7kZNy7LyGBz0UIuIImF
  # pF6A3bzzrR4wdacFY9U0vmqFXXcepxuT5p2UyAxwbLeDAgMBAAGjgcMwgcAwDAYD
  # VR0TAQH/BAIwADAdBgNVHQ4EFgQURLVVKanZPoXGEfYr1HmlaCEoD54wgYAGA1Ud
  # IwR5MHeAFES1VSmp2T6FxhH2K9R5pWghKA+eoUmkRzBFMRAwDgYDVQQKDAdQb3N0
  # SG9nMRUwEwYDVQQLDAxPbmVMb2dpbiBJZFAxGjAYBgNVBAMMEU9uZUxvZ2luIEFj
  # Y291bnQgghR2uIejyqYBTUjSDGwjtwVkeadrfzAOBgNVHQ8BAf8EBAMCB4AwDQYJ
  # KoZIhvcNAQEFBQADggEBALP5lhlcV8avbnVnqO7PBtlS2mVOJ2B7obm50OaJCbRh
  # t0I/dcNssWhT31/zmtNfKtrFicNImlKhdirApxpIp1WLEFY01a40GLmO6FG/WVvB
  # EzwXonWP+cP8jYQnqZ15JkuHjP3DYJuOak2GqAJAfaGO67q6IkRZzRq6UwEUgNJD
  # TlcsJAFaJDrcw07TY3mRFragdzGC7Xt/CM6r/0seY3+VBwMUMiJlvawcyQxap7om
  # EdgmQkJA8Dk6f+geI+U7jV3orkPiofBJi9K6cp5Fd9usut8jwi3GYg2wExNGbhF4
  # wlMD1LOhymQGBnTXPk+000nkBnYdqEnqXzVpDiCG1Pc=
  x509_cert:
  # -- Name of attribute that contains the permanent ID of the user in SAML assertions.
  # attr_permanent_id: "nameID"
  attr_permanent_id:
  # -- Name of attribute that contains the first name of the user in SAML assertions.
  # attr_first_name: "firstName"
  attr_first_name:
  # -- Name of attribute that contains the last name of the user in SAML assertions.
  # attr_last_name: "lastName"
  attr_last_name:
  # -- Name of attribute that contains the email of the user in SAML assertions.
  # attr_email: "email"
  attr_email:

# -- Name of the service and what port to expose on the pod. Don't change these unless you know what you're doing
service:
  name: posthog
  type: NodePort
  externalPort: 8000
  internalPort: 8000

  ## Service annotations
  ##
  annotations: {}

  ## External IP addresses of service
  ## Default: nil
  ##
  # externalIPs:
  # - 192.168.0.1
  ## Load Balancer allow-list
  # loadBalancerSourceRanges: []

cert-manager:
  # -- Whether to install cert-manager. Validates certs for nginx ingress
  enabled: false
  # -- Whether to install cert-manager CRDs.
  installCRDs: true

  #
  # [Workaround] - do not use the local DNS for the 'cert-manager' pods since it would return local IPs
  # and break self checks.
  #
  # For more info see:
  #   - https://github.com/jetstack/cert-manager/issues/1292
  #   - https://github.com/jetstack/cert-manager/issues/3238
  #   - https://github.com/jetstack/cert-manager/issues/4286
  #   - https://github.com/compumike/hairpin-proxy
  #
  # This has some side effects, like 'cert-manager' pods not being able to resolve cluster-local names,
  # but so far this has not caused issues (and we don't expect it to do so).
  #
  podDnsPolicy: None
  podDnsConfig:
    nameservers:
      - 8.8.8.8
      - 1.1.1.1
      - 208.67.222.222

ingress:
  # -- Enable ingress controller resource
  enabled: true
  # -- Ingress handler type. Defaults to `nginx` if nginx is enabled and to `clb` on gcp.
  type:
  # -- URL to address your PostHog installation. You will need to set up DNS after installation
  hostname:
  gcp:
    # -- Specifies the name of the global IP address resource to be associated with the google clb
    ip_name: "posthog"
    # -- If true, will force a https redirect when accessed over http
    forceHttps: true
    # -- Specifies the name of the tls secret to be used by the ingress. If not specified a managed certificate will be generated.
    secretName: ""
  # -- Whether to enable letsencrypt. Defaults to true if hostname is defined and nginx and cert-manager are enabled otherwise false.
  letsencrypt:
  nginx:
    # -- Whether nginx is enabled
    enabled: false
    # -- Whether to redirect to TLS with nginx ingress.
    redirectToTLS: true
  # -- Extra annotations
  annotations: {}
  # -- TLS secret to be used by the ingress.
  secretName:

postgresql:
  # -- Install postgres server on kubernetes (see below)
  enabled: true
  # -- Name override for postgresql app
  nameOverride: posthog-postgresql
  # -- Postgresql database name
  postgresqlDatabase: posthog
  # -- Postgresql database username
  postgresqlUsername: postgres
  # -- Postgresql database password
  postgresqlPassword: postgres
  persistence:
    # -- Enable persistence using PVC
    enabled: true
    # -- PVC Storage Request for PostgreSQL volume
    size: 10Gi
  # -- Host postgres is accessible from. Only set when internal PG is disabled
  postgresqlHost:
  # -- Host postgres is accessible from. Only set when internal PG is disabled
  postgresqlPort:
  # postgresqlHost: postgres
  # postgresqlPort: 5432
  # When defined the `postgresqlPassword` field is ignored
  # existingSecret: secret-name
  # existingSecretKey: postgresql-password

redis:
  # -- Install redis server on kubernetes (see below)
  enabled: true
  # -- Name override for redis app
  nameOverride: posthog-redis

  # -- Either standalone or cluster.
  architecture: standalone

  auth:
    # -- Don't require password by default
    enabled: false
    # Just omit the password field if your redis cluster doesn't use password
    # password: redis
    # When defined the `password` field is ignored
    # existingSecret: secret-name
    # existingSecretKey: redis-password

  # -- Host redis is accessible from. Only set when internal redis is disabled
  host:
  # -- Password for redis. Only set when internal redis is disabled
  password:
  # -- Port redis is accessible from. Only set when internal redis is disabled
  port:

  master:
    persistence:
      # -- Enable persistence using PVC
      enabled: true
      # -- PVC Storage Request for Redis volume
      size: 5Gi

kafka:
  # -- Install kafka on kubernetes
  enabled: true
  # -- Name override for kafka app
  nameOverride: posthog-kafka
  # -- URL for kafka. Only set when internal kafka is disabled
  url:
  # -- Host for kafka. Only set when internal kafka is disabled
  host:
  # -- Port for kafka. Only set when internal kafka is disabled
  port:
  service:
    enabled: false
    type: NodePort
  persistence:
    # -- Enable persistence using PVC
    enabled: true
    # -- PVC Storage Request for kafka volume
    size: 20Gi
  # -- A size-based retention policy for logs
  # -- Should be less than kafka.persistence.size, minimum 1GB
  logRetentionBytes: _15_000_000_000
  # -- The minimum age of a log file to be eligible for deletion due to age
  logRetentionHours: 24
  zookeeper:
    # -- Install zookeeper on kubernetes
    enabled: false
    # -- Name override for zookeeper app
  externalZookeeper:
    # -- URL for zookeeper. Only set when internal zookeeper is disabled
    # -- IF using default clickhouse zookeeper use <deployment-name>-posthog-zookeeper
    servers:
      - posthog-posthog-zookeeper:2181

zookeeper:
  # -- Install zookeeper on kubernetes
  enabled: true
  # -- Name override for zookeeper app
  nameOverride: posthog-zookeeper
  # -- replica count for zookeeper
  replicaCount: 1

clickhouse:
  # -- Use clickhouse as primary database
  enabled: true
  # -- Clickhouse database
  database: posthog
  # -- Clickhouse user
  user: admin
  # -- Clickhouse password
  password: a1f31e03-c88e-4ca6-a2df-ad49183d15d9
  # -- Set if not installing clickhouse operator
  host:
  # :TODO:
  replication: false
  secure: false
  verify: false
  async: false
  # externalZookeeper:
  # -- URL for zookeeper.
  # servers:
  # - host: posthog-posthog-zookeeper
  #   port: 2181
  # -- Optional: Used to manually specify a persistent volume claim. When specified the cloud specific storage class will not be provisioned
  persistentVolumeClaim:
  # -- Toleration labels for clickhouse pod assignment
  tolerations: []
  # -- Affinity settings for clickhouse pod
  affinity: {}
  # -- Clickhouse resource requests/limits. See more at http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  #   limits:
  #     cpu: 1000m
  #     memory: 16Gi
  #   requests:
  #     cpu: 4000m
  #     memory: 16Gi
  securityContext:
    enabled: true
    runAsUser: 101
    runAsGroup: 101
    fsGroup: 101

## Prometheus Exporter / Metrics
##
metrics:
  # -- Start an exporter for posthog metrics
  enabled: false

  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)

  # -- Metrics pods livenessProbe settings
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

  # -- Metrics pods readinessProbe settings
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 2
    failureThreshold: 3
    successThreshold: 1

  # -- Metrics resource requests/limits. See more at http://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
  #   limits:
  #     cpu: 100m
  #     memory: 100Mi
  #   requests:
  #     cpu: 100m
  #     memory: 100Mi

  # -- Node labels for metrics pod
  nodeSelector: {}
  # -- Toleration labels for metrics pod assignment
  tolerations: []
  # -- Affinity settings for metrics pod
  affinity: {}
  # schedulerName:
  # Optional extra labels for pod, i.e. redis-client: "true"
  # podLabels: []
  service:
    # --  Kubernetes service type for metrics service
    type: ClusterIP
    # -- Additional labels for metrics service
    labels: {}

  image:
    # -- Metrics exporter image repository
    repository: prom/statsd-exporter
    # -- Metrics exporter image tag
    tag: v0.10.5
    # -- Metrics exporter image pull policy
    pullPolicy: IfNotPresent

  # Enable this if you're using https://github.com/coreos/prometheus-operator
  serviceMonitor:
    # -- if `true`, creates a Prometheus Operator ServiceMonitor (also requires `metrics.enabled` to be `true`)
    enabled: false
    # -- Optional namespace which Prometheus is running in
    namespace:
    # -- How frequently to scrape metrics (use by default, falling back to Prometheus' default)
    interval:
    ## Defaults to what's used if you follow CoreOS [Prometheus Install Instructions](https://github.com/helm/charts/tree/master/stable/prometheus-operator#tldr)
    ## [Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#prometheus-operator-1)
    ## [Kube Prometheus Selector Label](https://github.com/helm/charts/tree/master/stable/prometheus-operator#exporters)
    # -- Default to kube-prometheus install (CoreOS recommended), but should be set according to Prometheus install
    selector:
      prometheus: kube-prometheus

cloudwatch:
  # -- Enable cloudwatch container insights to get logs and metrics on AWS
  enabled: false
  # -- AWS region
  region:
  # -- AWS EKS cluster name
  clusterName:
  # -- fluentBit configuration
  fluentBit:
    server: "On"
    port: 2020
    readHead: "On"
    readTail: "Off"

# Provide affinity for hooks if needed
hooks:
  # -- Affinity settings for hooks
  affinity: {}
  migrate:
    # -- Env variables for migate hooks
    env: []
    # -- Hook job resource limits/requests
    resources: {}

serviceAccount:
  # -- Configures if a ServiceAccount with this name should be created
  create: true
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  # -- name of the ServiceAccount to be used by access-controlled resources.
  # @default autogenerated
  name:
  # -- Configures annotation for the ServiceAccount
  annotations: {}

prometheus:
  # -- Whether to enable a minimal prometheus installation for getting alerts/monitoring the stack
  enabled: false

  alertmanager:
    # -- If false, alertmanager will not be installed
    enabled: true

    # -- alertmanager resource requests and limits
    resources:
      limits:
        cpu: 100m
      requests:
        cpu: 50m

  kubeStateMetrics:
    # -- If false, kube-state-metrics sub-chart will not be installed
    enabled: true

  nodeExporter:
    # -- If false, node-exporter will not be installed
    enabled: true

    # -- node-exporter resource limits & requests
    resources:
      limits:
        cpu: 100m
        memory: 50Mi
      requests:
        cpu: 50m
        memory: 30Mi

  pushgateway:
    # -- If false, pushgateway will not be installed
    enabled: false

  alertmanagerFiles:
    # -- alertmanager configuration rules. See https://prometheus.io/docs/alerting/latest/configuration/
    alertmanager.yml:
      global: {}
      receivers:
        - name: default-receiver
      route:
        group_by: [alertname]
        receiver: default-receiver

  serverFiles:
    # -- Alerts configuration, see https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
    alerting_rules.yml:
      groups:
        - name: Posthog alerts
          rules:
            # Alert for any pod that is down for >5 minutes.
            - alert: PodDown
              expr: up{job="kubernetes-pods"} == 0
              for: 1m
              labels:
                severity: alert
              annotations:
                summary: "Pod {{ $labels.kubernetes_pod_name }} down."
                description: "Pod {{ $labels.kubernetes_pod_name }} in namespace {{ $labels.kubernetes_namespace }} down for more than 5 minutes."
            - alert: PodFrequentlyRestarting
              expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
              for: 10m
              labels:
                severity: warning
              annotations:
                description:
                  Pod {{$labels.namespace}}/{{$labels.pod}} was restarted {{$value}}
                  times within the last hour
                summary: Pod is restarting frequently
            # Requires nodeExporter.enabled
            - alert: VolumeRemainingCapacityLowTest
              expr: kubelet_volume_stats_used_bytes/kubelet_volume_stats_capacity_bytes >= 0.85
              for: 5m
              labels:
                severity: page
              annotations:
                description: "Persistent volume claim {{ $labels.persistentvolumeclaim }} disk usage is above 85% for past 5 minutes"
                summary: "Kubernetes {{ $labels.persistentvolumeclaim }} is full (host {{ $labels.kubernetes_io_hostname }})"

# -- Prometheus StatsD configuration, see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-statsd-exporter
statsd:
  enabled: false

  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/path: /metrics
    prometheus.io/port: "9102"
